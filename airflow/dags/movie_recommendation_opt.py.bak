from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
import os
import logging
from als_optimization import optimize_als_parameters, evaluate_model_performance

# Configuration du DAG
default_args = {
    'owner': 'airflow',
    'depends_on_past': False,
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5),
}

dag = DAG(
    'movie_recommendation',
    default_args=default_args,
    description='DAG pour le système de recommandation de films',
    schedule_interval=None,
    start_date=datetime(2025, 5, 17),
    catchup=False,
)

def prepare_data(**context):
    """
    Prépare les données pour l'entraînement du modèle ALS
    - Charge les données depuis HDFS
    - Nettoie et transforme les données
    - Sauvegarde les données préparées
    """
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import col, when
    
    # Créer la session Spark
    spark = SparkSession.builder \
        .appName("Movie Recommendation - Data Preparation") \
        .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
        .getOrCreate()
    
    try:
        # Charger les données depuis les fichiers .dat
        ratings_df = spark.read \
            .option("delimiter", "::") \
            .option("header", "false") \
            .csv("hdfs://namenode:9000/user/airflow/movielens/ratings.dat") \
            .toDF("userId", "movieId", "rating", "timestamp")
        
        movies_df = spark.read \
            .option("delimiter", "::") \
            .option("header", "false") \
            .csv("hdfs://namenode:9000/user/airflow/movielens/movies.dat") \
            .toDF("movieId", "title", "genres")
        
        # Nettoyer les données
        # - Supprimer les doublons
        # - Filtrer les notes invalides
        # - Convertir les types
        clean_ratings = ratings_df \
            .dropDuplicates(['userId', 'movieId']) \
            .filter(col('rating').between(1, 5)) \
            .select(
                col('userId').cast('integer'),
                col('movieId').cast('integer'),
                col('rating').cast('float')
            )
        
        # Échantillonner 10% des données pour les tests
        clean_ratings = clean_ratings.sample(fraction=0.1, seed=42)
        
        # Sauvegarder les données préparées en parquet
        clean_ratings.write \
            .mode('overwrite') \
            .parquet("hdfs://namenode:9000/user/airflow/movielens/ratings_clean.parquet")
        
        # Afficher quelques statistiques
        logging.info(f"Nombre total de notes : {clean_ratings.count()}")
        logging.info(f"Nombre d'utilisateurs uniques : {clean_ratings.select('userId').distinct().count()}")
        logging.info(f"Nombre de films uniques : {clean_ratings.select('movieId').distinct().count()}")
        
    finally:
        spark.stop()

def train_als_model(**context):
    """
    Entraîne le modèle ALS avec les paramètres optimaux
    - Charge les données préparées
    - Configure et entraîne le modèle
    - Évalue les performances
    - Sauvegarde le modèle
    """
    from pyspark.sql import SparkSession
    from pyspark.ml.recommendation import ALS
    
    # Créer la session Spark avec des configurations optimisées
    spark = SparkSession.builder \
        .appName("Movie Recommendation - ALS Training") \
        .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
        .config("spark.driver.memory", "2g") \
        .config("spark.executor.memory", "2g") \
        .config("spark.driver.maxResultSize", "1g") \
        .config("spark.sql.shuffle.partitions", "10") \
        .config("spark.default.parallelism", "10") \
        .getOrCreate()
    
    try:
        # Charger les données préparées
        ratings_df = spark.read.parquet("hdfs://namenode:9000/user/airflow/movielens/ratings_clean.parquet")
        
        # Diviser en ensembles d'entraînement, validation et test
        train, validation, test = ratings_df.randomSplit([0.6, 0.2, 0.2], seed=42)
        
        # Optimiser les paramètres du modèle
        best_params, best_scores = optimize_als_parameters(train, validation, spark)
        
        # Créer le modèle ALS avec les meilleurs paramètres
        als = ALS(
            userCol="userId",
            itemCol="movieId",
            ratingCol="rating",
            coldStartStrategy="drop",
            nonnegative=True,
            **best_params['rmse']  # Utiliser les paramètres qui minimisent le RMSE
        )
        
        # Entraîner le modèle
        model = als.fit(train)
        
        # Évaluer les performances
        metrics = evaluate_model_performance(model, test, spark)
        
        # Afficher les résultats
        logging.info("Métriques d'évaluation :")
        for metric_name, value in metrics.items():
            logging.info(f"{metric_name}: {value}")
        
        # Sauvegarder le modèle
        model.save("hdfs://namenode:9000/user/airflow/movielens/als_model")
        
        # Sauvegarder les métriques
        spark.createDataFrame([metrics]).write \
            .mode('overwrite') \
            .parquet("hdfs://namenode:9000/user/airflow/movielens/model_metrics.parquet")
        
    finally:
        spark.stop()

def generate_recommendations(**context):
    """
    Génère les recommandations pour tous les utilisateurs
    - Charge le modèle entraîné
    - Génère les recommandations
    - Sauvegarde les résultats
    """
    from pyspark.sql import SparkSession
    from pyspark.ml.recommendation import ALSModel
    
    # Créer la session Spark
    spark = SparkSession.builder \
        .appName("Movie Recommendation - Generate Recommendations") \
        .config("spark.sql.warehouse.dir", "/tmp/spark-warehouse") \
        .getOrCreate()
    
    try:
        # Charger le modèle
        model = ALSModel.load("hdfs://namenode:9000/user/airflow/movielens/als_model")
        
        # Générer les recommandations pour tous les utilisateurs
        # (limité à 10 recommandations par utilisateur)
        recommendations = model.recommendForAllUsers(10)
        
        # Sauvegarder les recommandations en parquet
        recommendations.write \
            .mode('overwrite') \
            .parquet("hdfs://namenode:9000/user/airflow/movielens/recommendations.parquet")
        
        # Afficher quelques statistiques
        logging.info(f"Nombre d'utilisateurs avec recommandations : {recommendations.count()}")
        
    finally:
        spark.stop()

# Définir les tâches du DAG
prepare_data_task = PythonOperator(
    task_id='prepare_data',
    python_callable=prepare_data,
    dag=dag,
)

train_model_task = PythonOperator(
    task_id='train_als_model',
    python_callable=train_als_model,
    dag=dag,
)

generate_recommendations_task = PythonOperator(
    task_id='generate_recommendations',
    python_callable=generate_recommendations,
    dag=dag,
)

# Définir les dépendances
prepare_data_task >> train_model_task >> generate_recommendations_task 